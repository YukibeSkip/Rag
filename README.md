# 游뱄Sistema RAG游뱄
## 游댋 De que va el proyecto ?
El proyecto desarrolla diferentes sistemas rags:
-Uno que se comunica con una pagina web
-Y otro que se comunica con un pdf en espa침ol

### 游뽖游 Requisitos:
-install build-essential

-install langchain langchain_ollama

-install chromadb sentence-transformers langchain_huggingface langchain_chroma

-install gradio

-install bs4

# 游닉Descripci칩n de los scripts:

## 游늮游뱄 RAG:
### Importaci칩n de librer칤as
`requests`: Para obtener el contenido HTML de una p치gina web.

`BeautifulSoup`: Para analizar y extraer texto de la p치gina HTML.

`langchain_huggingface`: Para usar embeddings de Hugging Face para convertir el texto en vectores.

`langchain.text_splitter`: Para dividir el texto en fragmentos m치s peque침os (chunks) adecuados para el procesamiento.

`langchain_community.embeddings`: Para usar embeddings alternativos (en este caso, OllamaEmbeddings).

`langchain_chroma`: Para almacenar y buscar vectores de texto.

`langchain_ollama`.llms: Para usar un modelo de lenguaje de Ollama (en este caso, "llama3.2").

`langchain.schema`: Para definir documentos de texto.

`langchain.prompts`: Para crear plantillas de preguntas y respuestas (QA).

`langchain.chains`: Para construir la cadena de consulta y respuesta (RetrievalQA).

`os`: Para configurar variables de entorno, en este caso, para el agente de usuario (USER_AGENT).

###  Establecer el USER_AGENT
Se establece una variable de entorno `USER_AGENT` para simular un navegador espec칤fico al hacer solicitudes HTTP. Esto puede ayudar a evitar bloqueos por parte del servidor.
>>os.environ["USER_AGENT"] = "mi_usuario"

### Obtener el contenido de la p치gina web
-Se realiza una solicitud `GET` a la URL proporcionada, descargando el contenido HTML de la p치gina.

-Luego, se utiliza BeautifulSoup para analizar y extraer todo el texto de la p치gina.

###  Dividir el texto en fragmentos (chunks)
El texto extra칤do de la p치gina se divide en fragmentos m치s peque침os para facilitar su procesamiento, utilizando el `RecursiveCharacterTextSplitter`. Cada fragmento tiene un tama침o de hasta 1200 caracteres y se superpone un 10% (100 caracteres) entre ellos.


### Crear embeddings usando HuggingFaceEmbeddings
Se utiliza el modelo `sentence-transformers/all-MiniLM-L6-v2` de Hugging Face para crear representaciones vectoriales (embeddings) de los fragmentos de texto. Estas representaciones permiten buscar y comparar fragmentos de texto de manera eficiente.

### Crear documentos con los fragmentos
Cada fragmento de texto se convierte en un documento de la clase `Document de Langchain`. Esta clase es utilizada para almacenar el contenido de texto y las representaciones de los embeddings.

### Crear un vectorstore con Chroma
Utilizando el vectorstore `Chroma`, se almacenan los documentos y sus embeddings. Esto permite realizar b칰squedas eficientes basadas en similitud de vectores.

###  Definir el modelo LLM de Ollama
Se configura un modelo de lenguaje de Ollama, denominado `llama3.2`. Este modelo se utilizar치 para generar respuestas a las preguntas formuladas sobre el contenido del art칤culo.

### Crear un template de prompt para QA
Se define una plantilla de prompt para la consulta, en la que se proporciona el contexto (el texto relevante) y la pregunta a responder.

### Crear el `retriever` para la b칰squeda en el vectorstore





