# ğŸ¤–Sistema RAGğŸ¤–
## ğŸ”Œ De que va el proyecto ?
El proyecto desarrolla diferentes sistemas rags:
-Uno que se comunica con una pagina web
-Y otro que se comunica con un pdf en espaÃ±ol

### ğŸ©¸ğŸ’¾ Requisitos:
-install build-essential

-install langchain langchain_ollama

-install chromadb sentence-transformers langchain_huggingface langchain_chroma

-install gradio

-install bs4
## â˜ Antes de empezar
Antes hay que levantar Ollama en docker:

-docker network create ollama_network

-docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama --net=ollama_network ollama/ollama

# ğŸ“¢DescripciÃ³n de los scripts:

## ğŸ“ƒğŸ¤– RAG:
### ğŸ“šImportaciÃ³n de librerÃ­as
`requests`: Para obtener el contenido HTML de una pÃ¡gina web.

`BeautifulSoup`: Para analizar y extraer texto de la pÃ¡gina HTML.

`langchain_huggingface`: Para usar embeddings de Hugging Face para convertir el texto en vectores.

`langchain.text_splitter`: Para dividir el texto en fragmentos mÃ¡s pequeÃ±os (chunks) adecuados para el procesamiento.

`langchain_community.embeddings`: Para usar embeddings alternativos (en este caso, OllamaEmbeddings).

`langchain_chroma`: Para almacenar y buscar vectores de texto.

`langchain_ollama`.llms: Para usar un modelo de lenguaje de Ollama (en este caso, "llama3.2").

`langchain.schema`: Para definir documentos de texto.

`langchain.prompts`: Para crear plantillas de preguntas y respuestas (QA).

`langchain.chains`: Para construir la cadena de consulta y respuesta (RetrievalQA).

`os`: Para configurar variables de entorno, en este caso, para el agente de usuario (USER_AGENT).

###  ğŸ”Establecer el USER_AGENT
Se establece una variable de entorno `USER_AGENT` para simular un navegador especÃ­fico al hacer solicitudes HTTP. Esto puede ayudar a evitar bloqueos por parte del servidor.
>>os.environ["USER_AGENT"] = "mi_usuario"

### ğŸ“–Obtener el contenido de la pÃ¡gina web
-Se realiza una solicitud `GET` a la URL proporcionada, descargando el contenido HTML de la pÃ¡gina.

-Luego, se utiliza BeautifulSoup para analizar y extraer todo el texto de la pÃ¡gina.

### ğŸ“–âœDividir el texto en fragmentos (chunks)
El texto extraÃ­do de la pÃ¡gina se divide en fragmentos mÃ¡s pequeÃ±os para facilitar su procesamiento, utilizando el `RecursiveCharacterTextSplitter`. Cada fragmento tiene un tamaÃ±o de hasta 1200 caracteres y se superpone un 10% (100 caracteres) entre ellos.

### ğŸ“šğŸ¤—Crear embeddings usando HuggingFaceEmbeddings
Se utiliza el modelo `sentence-transformers/all-MiniLM-L6-v2` de Hugging Face para crear representaciones vectoriales (embeddings) de los fragmentos de texto. Estas representaciones permiten buscar y comparar fragmentos de texto de manera eficiente.

### âœğŸ““Crear documentos con los fragmentos
Cada fragmento de texto se convierte en un documento de la clase `Document de Langchain`. Esta clase es utilizada para almacenar el contenido de texto y las representaciones de los embeddings.

### ğŸCrear un vectorstore con Chroma
Utilizando el vectorstore `Chroma`, se almacenan los documentos y sus embeddings. Esto permite realizar bÃºsquedas eficientes basadas en similitud de vectores.

### ğŸ¦™ğŸ”¥ Definir el modelo LLM de Ollama
Se configura un modelo de lenguaje de Ollama, denominado `llama3.2`. Este modelo se utilizarÃ¡ para generar respuestas a las preguntas formuladas sobre el contenido del artÃ­culo.

### ğŸ“šCrear un template de prompt para QA
Se define una plantilla de prompt para la consulta, en la que se proporciona el contexto (el texto relevante) y la pregunta a responder.

### ğŸ•µï¸â€â™€ï¸Crear el `retriever` para la bÃºsqueda en el vectorstore
El `retriever` se utiliza para buscar documentos en el vectorstore. En este caso, se busca el documento mÃ¡s relevante a partir de la similitud de los embeddings. Se configuran los parÃ¡metros de bÃºsqueda para recuperar los 3 fragmentos mÃ¡s relevantes `(k=3)`.

### â˜ğŸ¤“Realizar la consulta y obtener la respuesta
Se realiza una consulta sobre el contenido del artÃ­culo, en este caso preguntando por los perÃ­odos de sobreventa y sobrecompra (usando una pregunta especÃ­fica). El retriever obtiene los fragmentos mÃ¡s relevantes y genera una respuesta.


## ğŸ¤–ğŸ² Rag en espaÃ±ol:
###  ğŸ“šImportaciÃ³n de librerÃ­as
`requests`: Aunque no se utiliza en este cÃ³digo, es una librerÃ­a que permite hacer solicitudes HTTP.

`PyPDF2`: Utilizada para leer y extraer texto de archivos PDF.

`langchain_huggingface`: Para utilizar embeddings de Hugging Face y convertir texto en vectores.

`langchain.text_splitter`: Para dividir el texto en fragmentos mÃ¡s pequeÃ±os y manejables.

`langchain_ollama.llms`: Para utilizar un modelo de lenguaje de Ollama (en este caso, "llama3.2").

`langchain_chroma`: Para almacenar y recuperar vectores de texto eficientemente.

`langchain.schema`: Para crear documentos de texto que se procesan en Langchain.

`langchain.prompts`: Para generar plantillas de preguntas y respuestas.

`langchain.chains`: Para construir una cadena de consulta y respuesta (RetrievalQA).

`os`: Para manejar configuraciones del sistema operativo, como directorios.

### ğŸ““ğŸ²Leer y extraer texto del archivo PDF
El cÃ³digo abre un archivo PDF ubicado en la ruta `pdf_path` y extrae el texto de cada una de sus pÃ¡ginas utilizando `PyPDF2`. El texto extraÃ­do se guarda en una variable `text`.

### ğŸ“–âœDividir el texto en fragmentos (chunks)
El texto extraÃ­do del PDF se divide en fragmentos mÃ¡s pequeÃ±os utilizando `RecursiveCharacterTextSplitter`. Cada fragmento tiene un tamaÃ±o mÃ¡ximo de 500 caracteres y se superpone un 10% (50 caracteres) con el fragmento anterior.

### âš™ğŸ”¥ğŸ¦™ Configurar el modelo de lenguaje de Ollama
Se crea una instancia del modelo de lenguaje `llama3.2` de Ollama, que se utilizarÃ¡ para generar respuestas basadas en el contexto de los fragmentos de texto.

### ğŸ“šğŸ¤— Cargar el modelo de embeddings de HuggingFace
Se carga el modelo de embeddings `sentence-transformers/all-MiniLM-L6-v2` de Hugging Face para convertir el texto de cada fragmento en una representaciÃ³n vectorial (embedding). Los embeddings son representaciones numÃ©ricas del texto que permiten realizar bÃºsquedas eficientes.

### ğŸInicializar el vectorstore con Chroma
Se crea un vectorstore (almacen de vectores) utilizando la librerÃ­a `Chroma`. Este almacÃ©n guarda las representaciones vectoriales de los fragmentos de texto, lo que facilita la bÃºsqueda y recuperaciÃ³n eficiente basada en similitud de vectores

### ğŸ“šCrear documentos de Langchain a partir de los fragmentos
Los fragmentos de texto se convierten en documentos de la clase `Document` de Langchain. Esta clase es utilizada para almacenar tanto el contenido de texto como sus representaciones de embeddings.

### ğŸ¤œğŸ“š Agregar los documentos al vectorstore
Se agregan los documentos (y sus embeddings) al vectorstore `Chroma`, lo que permite que luego se realicen bÃºsquedas en estos documentos utilizando similitudes de embeddings.

### â˜ğŸ¤“ Crear una plantilla de prompt para preguntas y respuestas
Se crea una plantilla de prompt para la consulta utilizando la clase `PromptTemplate`. La plantilla tiene un marcador de posiciÃ³n para el contexto (el fragmento de texto relevante) y otro para la pregunta del usuario.

### ğŸ•µï¸â€â™€ï¸Crear un retriever para buscar en el vectorstore
Se crea un `retriever` a partir del vectorstore `Chroma`. El `retriever` es responsable de realizar bÃºsquedas basadas en la similitud de los embeddings, y en este caso, se configura para buscar los 3 fragmentos mÃ¡s relevantes para cada consulta.

### â˜ğŸ¤“ğŸ§â€â™€ï¸Realizar la consulta y obtener la respuesta
Se define una pregunta de ejemplo `("Â¿CuÃ¡nto viven los elfos?")` y se pasa al retriever para buscar los fragmentos mÃ¡s relevantes. DespuÃ©s, se obtiene la respuesta usando el modelo LLM y se imprime el resultado.

